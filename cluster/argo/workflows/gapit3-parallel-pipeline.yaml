apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: gapit3-gwas-parallel-  # Argo appends unique suffix
  namespace: runai-talmo-lab
  labels:
    pipeline: gapit3-gwas
    stage: production
spec:
  # ===========================================================================
  # GAPIT3 GWAS - Full Parallel Pipeline (186 Traits)
  # ===========================================================================
  # Production workflow: runs all 186 traits in parallel
  # DAG structure: Validate → Extract → 186 parallel GWAS → Collect Results
  #
  # NOTE: Resource allocation (CPU, memory) is configured in WorkflowTemplate:
  #   cluster/argo/workflow-templates/gapit3-single-trait-template.yaml
  # ===========================================================================

  entrypoint: parallel-pipeline
  serviceAccountName: default

  # Workflow-level parameters
  # Parameter naming follows GAPIT v3.0.0 conventions (see docs/GAPIT_PARAMETERS.md)
  arguments:
    parameters:
    - name: image
      value: "ghcr.io/salk-harnessing-plants-initiative/gapit3-gwas-pipeline:sha-539fe5c-test"
    - name: data-hostpath
      value: "/hpi/hpi_dev/users/eberrigan/20251208_Elohim_Bello_iron_deficiency_GAPIT_GWAS/data"
    - name: output-hostpath
      value: "/hpi/hpi_dev/users/eberrigan/20251208_Elohim_Bello_iron_deficiency_GAPIT_GWAS/outputs"
    # Data file paths (relative to /data mount point)
    - name: genotype-file
      value: "/data/genotype/acc_snps_filtered_maf_perl_edited_diploid.hmp.txt"
    - name: phenotype-file
      value: "/data/phenotype/iron_traits_edited.txt"
    - name: accession-ids-file
      value: "/data/metadata/ids_gwas.txt"
    # Trait range
    - name: start-trait-index
      value: "2"      # First trait column (after Taxa)
    - name: end-trait-index
      value: "187"    # Last trait column (186 traits total)
    # GAPIT parameters (v3.0.0 naming)
    - name: model
      value: "BLINK,FarmCPU,MLM"
    - name: pca-total
      value: "3"
    - name: snp-maf
      value: "0"
    - name: snp-fdr
      value: "0.05"  # 5% FDR threshold (Benjamini-Hochberg correction)

  # Global timeout (7 days - effectively no limit, can terminate manually if needed)
  activeDeadlineSeconds: 604800

  # Workflow-level volumes
  volumes:
  - name: nfs-data
    hostPath:
      path: "{{workflow.parameters.data-hostpath}}"
      type: Directory
  - name: nfs-outputs
    hostPath:
      path: "{{workflow.parameters.output-hostpath}}"
      type: DirectoryOrCreate

  templates:
  # ===========================================================================
  # Main DAG
  # ===========================================================================
  - name: parallel-pipeline
    dag:
      tasks:
      # Step 1: Validate inputs
      - name: validate-inputs
        template: validate

      # Step 2: Extract trait manifest
      - name: extract-traits
        dependencies: [validate-inputs]
        template: extract-traits-inline

      # Step 3: Run all traits in parallel (withSequence)
      - name: run-all-traits
        dependencies: [extract-traits]
        templateRef:
          name: gapit3-gwas-single-trait
          template: run-gwas
        arguments:
          parameters:
          - name: trait-index
            value: "{{item}}"
          - name: trait-name
            value: "trait-{{item}}"
          - name: image
            value: "{{workflow.parameters.image}}"
          - name: genotype-file
            value: "{{workflow.parameters.genotype-file}}"
          - name: phenotype-file
            value: "{{workflow.parameters.phenotype-file}}"
          - name: accession-ids-file
            value: "{{workflow.parameters.accession-ids-file}}"
          - name: model
            value: "{{workflow.parameters.model}}"
          - name: pca-total
            value: "{{workflow.parameters.pca-total}}"
          - name: snp-maf
            value: "{{workflow.parameters.snp-maf}}"
          - name: snp-fdr
            value: "{{workflow.parameters.snp-fdr}}"
        withSequence:
          start: "{{workflow.parameters.start-trait-index}}"
          end: "{{workflow.parameters.end-trait-index}}"

      # Step 4: Collect and aggregate results (runs after all traits complete)
      - name: collect-results
        dependencies: [run-all-traits]
        templateRef:
          name: gapit3-results-collector
          template: collect-results
        arguments:
          parameters:
          - name: image
            value: "{{workflow.parameters.image}}"
          - name: output-hostpath
            value: "{{workflow.parameters.output-hostpath}}"
          - name: batch-id
            value: "{{workflow.name}}"

  # ===========================================================================
  # Template: Validation
  # ===========================================================================
  - name: validate
    container:
      image: "{{workflow.parameters.image}}"
      imagePullPolicy: Always
      command: ["/scripts/entrypoint.sh"]
      args: ["validate"]

      env:
      - name: GENOTYPE_FILE
        value: "{{workflow.parameters.genotype-file}}"
      - name: PHENOTYPE_FILE
        value: "{{workflow.parameters.phenotype-file}}"
      - name: ACCESSION_IDS_FILE
        value: "{{workflow.parameters.accession-ids-file}}"

      volumeMounts:
      - name: nfs-data
        mountPath: /data
        readOnly: true

      resources:
        requests:
          memory: "2Gi"
          cpu: "1"
        limits:
          memory: "4Gi"
          cpu: "2"

  # ===========================================================================
  # Template: Extract traits
  # ===========================================================================
  - name: extract-traits-inline
    container:
      image: "{{workflow.parameters.image}}"
      imagePullPolicy: Always
      command: ["/scripts/entrypoint.sh"]
      args:
        - "extract-traits"
        - "{{workflow.parameters.phenotype-file}}"
        - "/outputs/traits_manifest.yaml"

      volumeMounts:
      - name: nfs-data
        mountPath: /data
        readOnly: true
      - name: nfs-outputs
        mountPath: /outputs

      resources:
        requests:
          memory: "2Gi"
          cpu: "1"
        limits:
          memory: "4Gi"
          cpu: "2"

  # ===========================================================================
  # Parallelism control
  # ===========================================================================
  # Limit concurrent pod execution to avoid overwhelming cluster
  # At peak: 30 jobs × 64GB = 1.9TB RAM, 30 jobs × 12 CPU = 360 CPUs
  parallelism: 30
