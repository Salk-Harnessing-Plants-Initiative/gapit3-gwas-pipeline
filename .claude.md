# GAPIT3 GWAS Pipeline - Project Context

## Project Overview

This is a **production-ready, parallelized GWAS (Genome-Wide Association Study) pipeline** for analyzing 184 iron traits in Arabidopsis thaliana using GAPIT3 v3.5.0 and Argo Workflows.

**Key Achievement**: Transforms a manual R script into a scalable, cloud-native pipeline with 50-100x speedup through parallel execution.

## Repository Information

- **GitHub**: https://github.com/Salk-Harnessing-Plants-Initiative/gapit3-gwas-pipeline
- **Container Registry**: `ghcr.io/salk-harnessing-plants-initiative/gapit3-gwas-pipeline`
- **Primary Language**: R (with GAPIT3 package)
- **Orchestration**: Argo Workflows (Kubernetes)
- **User**: Elizabeth (eberrigan)
- **Collaborator**: Elohim Bello Bello (Salk Institute) - provided original script, does not code

## Dataset Specifications

- **Species**: Arabidopsis thaliana
- **Accessions**: 546
- **SNPs**: ~1.4 million (HapMap format)
- **Traits**: 184 iron-related traits (root morphology)
- **Genotype File**: `acc_snps_filtered_maf_perl_edited_diploid.hmp.txt` (~2.3GB)
- **Phenotype File**: `mean_median_root_length_day2.txt`
- **Models**: BLINK, FarmCPU
- **PCA Components**: 3

## Architecture

### Pipeline Components

```
┌─────────────────────────────────────────────────────────────┐
│                     Argo Workflows DAG                       │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  ┌─────────────┐    ┌──────────────────────────────────┐   │
│  │  Validate   │───▶│   Extract Trait Manifest         │   │
│  │   Inputs    │    │   (184 trait columns)            │   │
│  └─────────────┘    └──────────────────────────────────┘   │
│                                    │                         │
│                                    ▼                         │
│              ┌────────────────────────────────────┐         │
│              │  Parallel GWAS Execution           │         │
│              │  ┌──────┐  ┌──────┐  ┌──────┐     │         │
│              │  │Trait1│  │Trait2│  │ ...  │     │         │
│              │  │32GB  │  │32GB  │  │32GB  │     │         │
│              │  │12CPU │  │12CPU │  │12CPU │     │         │
│              │  └──────┘  └──────┘  └──────┘     │         │
│              │  Parallelism: 50 (configurable)   │         │
│              └────────────────────────────────────┘         │
│                                    │                         │
│                                    ▼                         │
│              ┌────────────────────────────────────┐         │
│              │  Collect & Aggregate Results       │         │
│              │  - Significant SNPs                │         │
│              │  - Summary statistics              │         │
│              │  - Manhattan/QQ plots              │         │
│              └────────────────────────────────────┘         │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### Computational Requirements

**Per Trait:**
- Memory: 32GB RAM
- CPU: 12 cores
- Duration: ~5-10 minutes
- Total for 184 traits: ~15-30 hours serial, **3-4 hours parallel** (50 jobs)

### Technology Stack

- **Base Image**: `rocker/r-ver:4.4.1`
- **R Packages**: GAPIT3, data.table, dplyr, tidyr, ggplot2, optparse, yaml
- **Optimization**: OpenBLAS multi-threaded linear algebra
- **Container**: Docker multi-stage builds
- **CI/CD**: GitHub Actions → GHCR
- **Orchestration**: Argo Workflows v1alpha1
- **Storage**: NFS via hostPath volumes (single-user cluster)

## File Structure (Important Paths)

```
gapit3-gwas-pipeline/
├── .github/workflows/
│   └── docker-build.yml          # CI/CD: test vs prod tags, verification
├── cluster/argo/
│   ├── workflow-templates/       # Reusable Argo templates
│   │   ├── gapit3-single-trait-template.yaml
│   │   ├── trait-extractor-template.yaml
│   │   └── results-collector-template.yaml
│   ├── workflows/                # Workflow definitions
│   │   ├── gapit3-test-pipeline.yaml        # 3 traits for testing
│   │   └── gapit3-parallel-pipeline.yaml    # Full 184 traits
│   └── scripts/
│       ├── submit_workflow.sh    # Helper to submit workflows
│       └── monitor_workflow.sh   # Monitor workflow progress
├── scripts/
│   ├── run_gwas_single_trait.R   # Core GWAS execution (CLI-enabled)
│   ├── validate_inputs.R         # Pre-flight validation
│   ├── extract_trait_names.R     # Generate trait manifest
│   ├── collect_results.R         # Aggregate results
│   └── entrypoint.sh             # Container entrypoint
├── config/
│   └── config.yaml               # GAPIT parameters (centralized)
├── docs/
│   ├── ARGO_SETUP.md            # Cluster deployment guide
│   └── DOCKER_WORKFLOW.md        # CI/CD guide
├── Dockerfile                    # Multi-stage build with R + GAPIT3
├── gwas_using_gapit3_original.R  # Original script (REFERENCE ONLY)
├── README.md                     # Main documentation
└── QUICKSTART.md                 # Non-technical user guide
```

## Important Files to NEVER Modify

### `gwas_using_gapit3_original.R`
- **Purpose**: Original R console session from collaborator
- **Status**: Reference-only, contains console prompts (`>`), not executable
- **Why**: Historical record of collaborator's workflow
- **Production version**: See `scripts/run_gwas_single_trait.R`

## Key Design Decisions

### 1. Parallelization Strategy
**Decision**: One trait per job (184 independent jobs)
**Rationale**: Original script ran all traits serially → 50+ GB RAM. Per-trait execution needs only 32GB.
**Result**: 50-100x speedup with parallel execution

### 2. Argo vs Run.AI
**Decision**: Use Argo Workflows instead of Run.AI CLI
**Rationale**: User showed preference with SLEAP-Roots pipeline pattern. Better fault tolerance, resumability, visualization.
**Pattern**: Followed `talmolab/sleap-roots-pipeline` architecture

### 3. HostPath Volumes
**Decision**: Use hostPath instead of PersistentVolumeClaims
**Rationale**: Single-user cluster, NFS already mounted at known paths
**Security Note**: Documented as acceptable for single-user context

### 4. Test vs Production Tags
**Decision**: Separate `-test` suffix for development images
**Rationale**: Clear distinction between stable and testing versions
**Implementation**:
- Pull requests → `pr-1-test`, `sha-abc123-test`
- Main branch → `main-test`, `sha-abc123-test`
- Version tags → `v1.0.0`, `latest` (no suffix)

### 5. Path Filtering in CI
**Decision**: Only build on relevant file changes
**Paths Watched**:
- `Dockerfile`
- `scripts/**`
- `config/**`
- `.github/workflows/docker-build.yml`
**Rationale**: Avoid unnecessary builds on doc changes

## Docker Image Details

### Build Process
1. **Base stage**: R 4.4.1 + system dependencies
2. **Layer 1**: Core R packages (data.table, dplyr, tidyr, readr, devtools)
3. **Layer 2**: Visualization packages (ggplot2, gridExtra, etc.)
4. **Layer 3**: GAPIT3 from GitHub (`jiabowang/GAPIT@master`)
5. **Runtime stage**: Copy scripts, set entrypoint

### System Dependencies (Critical)
- `pkg-config` - Required for ragg package compilation
- `libwebp-dev` - Required for ragg package
- `libharfbuzz-dev`, `libfribidi-dev` - Text rendering
- `libopenblas-dev` - Multi-threaded linear algebra

### Image Verification Tests
CI automatically verifies:
1. R installation (`R --version`)
2. GAPIT3 loads (`library(GAPIT)`)
3. Required packages available
4. Entrypoint help command works
5. Validation script runs

### Current Images
- **Test**: `ghcr.io/salk-harnessing-plants-initiative/gapit3-gwas-pipeline:main-test`
- **Production**: Not yet tagged (waiting for v1.0.0 release)

## Argo Workflow Parameters

### Configurable Parameters
- `trait-index` - Which trait to analyze (1-186)
- `start-trait-index` - Starting trait for batch (default: 1)
- `end-trait-index` - Ending trait for batch (default: 186)
- `max-parallelism` - Concurrent jobs (default: 50)
- `data-hostpath` - NFS path to input data
- `outputs-hostpath` - NFS path for results

### User-Configurable Placeholders
All Argo manifests contain TODOs for:
- `namespace: default` → Change to user's namespace
- `/hpi/hpi_dev/users/YOUR_USERNAME/gapit3-gwas` → Replace with actual path

## Common Issues & Solutions

### Issue 1: Docker Build Failures
**Symptoms**: `ragg` package compilation fails
**Solution**: Ensure `pkg-config` and `libwebp-dev` are installed
**Files**: `Dockerfile:23` (system dependencies)

### Issue 2: R Operator Errors
**Symptoms**: `%R%` or `%||%` undefined
**Solution**: Already fixed - `%R%` → `strrep()`, `%||%` → `if/else`
**Status**: Addressed in commit `b5a0254`

### Issue 3: Verification Tests Fail
**Symptoms**: Container entrypoint intercepts R commands
**Solution**: Use `--entrypoint R` to bypass wrapper
**Files**: `.github/workflows/docker-build.yml:188-211`

### Issue 4: Path Issues on Cluster
**Symptoms**: "Directory not found" in Argo pods
**Solution**: Update hostPath values in WorkflowTemplates
**Files**: `cluster/argo/workflow-templates/*.yaml` (parameters section)

## Development Workflow

### Making Changes
1. Create feature branch: `git checkout -b feat/your-feature`
2. Make changes
3. Test locally with devcontainer (`.devcontainer/devcontainer.json`)
4. Push and create PR
5. CI runs automatically (build + verification)
6. After merge, CI builds test image on main

### Creating Production Release
```bash
git checkout main
git pull origin main
git tag -a v1.0.0 -m "Release v1.0.0: Description"
git push origin v1.0.0
```
This triggers production Docker build with clean tags.

### Testing on Cluster
```bash
# 1. Update paths in Argo manifests
# 2. Install templates
kubectl apply -f cluster/argo/workflow-templates/

# 3. Run test (3 traits)
kubectl apply -f cluster/argo/workflows/gapit3-test-pipeline.yaml

# 4. Monitor
./cluster/argo/scripts/monitor_workflow.sh

# 5. Check results
ls /path/to/outputs/test-*/
```

## Git Workflow

### Branch Strategy
- `main` - Production-ready code
- `feat/*` - Feature branches
- `fix/*` - Bug fixes

### Commit Message Format
```
type: Short description

Longer explanation if needed.

- Bullet points for details
- Multiple changes listed

Fixes #issue (if applicable)
```

Types: `feat`, `fix`, `docs`, `refactor`, `test`, `chore`

### Pull Request Process
1. Create PR with clear description
2. CI automatically runs (build + verify)
3. GitHub Copilot reviews code (may need manual resolution)
4. Merge to main after approval
5. Delete feature branch

## FAIR Principles Implementation

### Findable
- Structured outputs with timestamps
- Metadata tracking in JSON format
- GitHub repository with DOI (future)

### Accessible
- Public GitHub repository
- Public container registry (GHCR)
- Comprehensive documentation

### Interoperable
- Standard formats: HapMap (genotype), CSV (phenotype), JSON (metadata)
- Docker containers (portable)
- Argo Workflows (cloud-native)

### Reusable
- Versioned releases
- Complete documentation
- Configurable parameters
- Example workflows

## Future Enhancements (Roadmap)

### High Priority
1. **Monitoring & Alerting**
   - Prometheus metrics export
   - Grafana dashboard
   - Slack/email notifications
   - Failed trait retry logic

2. **Testing Infrastructure**
   - Synthetic test data for CI
   - R unit tests (testthat)
   - Integration tests

### Medium Priority
3. **Visualization Dashboard**
   - Aggregated Manhattan plots
   - Interactive Shiny/Plotly dashboard
   - Trait correlation heatmap

4. **Performance Optimization**
   - Auto-scaling based on cluster load
   - Memory profiling
   - Result caching

### Low Priority
5. **Additional Features**
   - Support for additional GAPIT models
   - Covariate support
   - Multi-species support

## Troubleshooting

### CI Failing?
1. Check GitHub Actions logs: `gh run view <run-id> --log-failed`
2. Common issues: Dockerfile syntax, missing dependencies, path errors
3. Test locally: Build Docker image and run verification commands

### Argo Workflow Failing?
1. Check pod logs: `kubectl logs <pod-name>`
2. Check workflow status: `kubectl get workflow`
3. Describe workflow: `kubectl describe workflow <workflow-name>`
4. Common issues: Path permissions, resource limits, missing data files

### R Script Errors?
1. Check metadata.json for error details
2. Run locally in devcontainer for debugging
3. Verify input files are readable
4. Check GAPIT3 version compatibility

## Contact & Collaboration

- **Primary Developer**: Elizabeth (eberrigan)
- **Collaborator**: Elohim Bello Bello (Salk Institute)
- **Institution**: Salk Institute for Biological Studies
- **Project**: Harnessing Plants Initiative

## Important Reminders

### Do NOT Modify:
- `gwas_using_gapit3_original.R` - Keep as historical reference
- Original GAPIT logic in `scripts/run_gwas_single_trait.R` - Only add features, don't refactor core

### Always Update:
- Documentation when changing behavior
- CHANGELOG when releasing versions
- Workflow templates when changing parameters

### Before Merging:
- Run full CI pipeline
- Test on cluster if infrastructure changes
- Update documentation
- Address all review comments

## Recent Development History

### Merged PRs
- **PR #1** (Oct 25, 2024): Initial Argo Workflows implementation
  - 25 files, +3,779 lines
  - Docker + Argo + Scripts + Documentation
  - Addressed 24 GitHub Copilot review comments
  - All CI checks passing

### Current Branch Status
- **main**: Up to date, all features merged
- **feat/argo-workflows-implementation**: Can be deleted
- Latest commit: `bc10fc8` - "Implement Argo Workflows-based parallelized GAPIT3 GWAS pipeline (#1)"

### Next Milestone
- Test on actual cluster with real data
- Create v1.0.0 release after successful testing
- Run full 184-trait production pipeline

## Quick Reference Commands

```bash
# Check CI status
gh run list --limit 5

# View PR
gh pr view 1

# Check Docker image
docker pull ghcr.io/salk-harnessing-plants-initiative/gapit3-gwas-pipeline:main-test

# Test locally
docker run --rm ghcr.io/.../gapit3-gwas-pipeline:main-test --help

# Deploy to cluster
kubectl apply -f cluster/argo/workflow-templates/
kubectl apply -f cluster/argo/workflows/gapit3-test-pipeline.yaml

# Monitor workflow
kubectl get workflows -w
./cluster/argo/scripts/monitor_workflow.sh
```

---

**Last Updated**: October 25, 2024
**Status**: Production-ready, pending cluster testing
**Version**: Pre-release (awaiting v1.0.0 tag)
